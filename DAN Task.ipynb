{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9c64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilia/Deps/miniconda3/envs/cv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d7074",
   "metadata": {},
   "source": [
    "# Deep Average Network –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d972fe",
   "metadata": {},
   "source": [
    "–í —ç—Ç–æ–π –¥–æ–º–∞—à–∫–µ –º—ã –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–∏—Ç—ã –Ω–∞ 3 —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.  \n",
    "–í—ã –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤, —Ç–∞–∫ —á—Ç–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å [—Ç—É—Ç–æ—Ä–∏–∞–ª –ø–æ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é](https://github.com/BobaZooba/DeepNLP/blob/master/Tutorials/Word%20vectors%20%26%20Data%20Loading.ipynb).\n",
    "\n",
    "–ù–∞—à–∏ –∫–ª–∞—Å—Å—ã:  \n",
    "\n",
    "–ò–Ω–¥–µ–∫—Å | Sentiment  \n",
    "-- | --  \n",
    "0 | negative  \n",
    "1 | neutral  \n",
    "2 | positive  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55010212",
   "metadata": {},
   "source": [
    "–í–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–∞–∫—É—é –º–æ–¥–µ–ª—å:\n",
    "![–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ DAN](https://www.researchgate.net/profile/Shervin-Minaee/publication/340523298/figure/fig1/AS:878252264550411@1586403065555/The-architecture-of-the-Deep-Average-Network-DAN-10.ppm)\n",
    "\n",
    "–ß—Ç–æ –æ–Ω–∞ –∏–∑ —Å–µ–±—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç:\n",
    "- –ú—ã –ø–æ–¥–∞–µ–º –≤ –Ω–µ–µ –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤\n",
    "- –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ `Multilayer Perceptron`\n",
    "\n",
    "–í —ç—Ç–æ–π –¥–æ–º–∞—à–∫–µ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:\n",
    "- –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç—ã –≤ –º–∞—Ç—Ä–∏—Ü—ã —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å\n",
    "- –û–±—É—á–∏—Ç—å –µ–µ\n",
    "- –ü–æ–Ω—è—Ç—å —Ö–æ—Ä–æ—à–æ –ª–∏ –≤—ã —ç—Ç–æ —Å–¥–µ–ª–∞–ª–∏\n",
    "\n",
    "–≠—Ç–æ –æ—á–µ–Ω—å –≤–∞–∂–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏. –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–∞ —Ä–∞–±–æ—Ç–µ —Å–æ–≤–µ—Ç—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–∫—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –±–µ–π–∑–ª–∞–π–Ω. –ò –≤ –∫–∞—á–µ—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤ –≤–∑—è—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç –±–µ—Ä—Ç–∞/—Ä–æ–±–µ—Ä—Ç—ã/—Ç–¥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65faf753",
   "metadata": {},
   "source": [
    "## ü§ó Datasets\n",
    "–í —ç—Ç–æ–º —Ç—É—Ç–æ—Ä–∏–∞–ª–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ [datasets](https://github.com/huggingface/datasets). –ú—ã –≤—Ä—è–¥ –ª–∏ –µ—â–µ –±—É–¥–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —ç—Ç–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π, —Ç–∞–∫ –∫–∞–∫ –Ω–∞–º –±—É–¥–µ—Ç –≤–∞–∂–Ω–æ —Å–∞–º–∏–º –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –≤–æ-–≤—Ç–æ—Ä—ã—Ö, –∑–¥–µ—Å—å –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ–ø–ª–æ—Ö–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏. [–ó–¥–µ—Å—å](https://huggingface.co/datasets) –≤—ã —Å–º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–≥–¥–∞-–Ω–∏–±—É–¥—å –æ–Ω–∏ –≤–∞–º –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927f02a",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ –∑–∞–≥—Ä—É–∑–∫–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤ –∏ `np.array`\n",
    "–§–æ—Ä–º–∞—Ç —Å–ª–æ–≤–∞—Ä—è:\n",
    "```python\n",
    "{\n",
    "    'aabra': 0,\n",
    "    ...,\n",
    "    'mom': 6546,\n",
    "    ...\n",
    "    'xyz': 100355\n",
    "}\n",
    "```\n",
    "–§–æ—Ä–º–∞—Ç –º–∞—Ç—Ä–∏—Ü—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:\n",
    "```python\n",
    "array([[0.44442278, 0.28644582, 0.04357426, ..., 0.9425766 , 0.02024289,\n",
    "        0.88456545],\n",
    "       [0.77599317, 0.35188237, 0.54801261, ..., 0.91134102, 0.88599103,\n",
    "        0.88068835],\n",
    "       [0.68071886, 0.29352313, 0.95952505, ..., 0.19127958, 0.97723054,\n",
    "        0.36294011],\n",
    "       ...,\n",
    "       [0.03589378, 0.85429694, 0.33437761, ..., 0.39784873, 0.80368014,\n",
    "        0.76368042],\n",
    "       [0.01498725, 0.78155695, 0.80372969, ..., 0.82051826, 0.42314861,\n",
    "        0.18655465],\n",
    "       [0.69263802, 0.82090775, 0.27150426, ..., 0.86582747, 0.40896573,\n",
    "        0.33423976]])\n",
    "```\n",
    "\n",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –º–∞—Ç—Ä–∏—Ü–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ä–∞–∑–º–µ—Ä–æ–º —Å–ª–æ–≤–∞—Ä—è, —Ç–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥. –ü–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É `num_tokens` –¥–æ–ª–∂–Ω–æ –±—Ä–∞—Ç—å –Ω–µ –±–æ–ª–µ–µ —É–∫–∞–∑–∞–Ω–æ –≤ —ç—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä—å –∏ –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b2b68",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "–ú—ã —Å—Ä–∞–∑—É –ø–æ–ª—É—á–∏–º `torch.utils.data.Dataset`, –∫–æ—Ç–æ—Ä—ã–π —Å–º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –≤ `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54fdaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (/home/ilia/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (/home/ilia/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (/home/ilia/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"tweet_eval\"\n",
    "dataset_name = \"sentiment\"\n",
    "\n",
    "train_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"train\")\n",
    "valid_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"validation\")\n",
    "test_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a0650",
   "metadata": {},
   "source": [
    "## `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc742027",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9012ae5",
   "metadata": {},
   "source": [
    "## –ü–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –æ—Ç–¥–∞–µ—Ç –Ω–∞–º `Loader`\n",
    "–≠—Ç–æ –±–∞—Ç—á —Ñ–æ—Ä–º–∞—Ç–∞:\n",
    "```python\n",
    "batch = {\n",
    "    \"text\": [\n",
    "        \"text1\",\n",
    "        \"text2\",\n",
    "        ...,\n",
    "        \"textn\"\n",
    "    ],\n",
    "    \"label\": tensor([\n",
    "        1,\n",
    "        1,\n",
    "        ...,\n",
    "        0\n",
    "    ])\n",
    "}\n",
    "```\n",
    "–¢–æ –µ—Å—Ç—å —É –Ω–∞—Å –µ—Å—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å –¥–≤—É–º—è –∫–ª—é—á–∞–º–∏ `text` –∏ `label`, –≥–¥–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è n –ø—Ä–∏–º–µ—Ä–æ–≤. –¢–æ –µ—Å—Ç—å –¥–ª—è 5-–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ —Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ `batch[\"text\"][5]`, –∞ –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞ –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ `batch[\"label\"][5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bf6b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['C\\\\u2019mon Rebels\\\\u002c put an apple in the pigs mouth!!!   Red Wolves fans focus all of your hate & anger at the pigs tomorrow.',\n",
       "  \"Hey happy Friday!!! Today T&amp;P will be giving away a pair of tickets to the Zac Brown Band's August 7th show at...\"],\n",
       " 'label': tensor([0, 2])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9f0e6",
   "metadata": {},
   "source": [
    "## Collate\n",
    "–°–µ–π—á–∞—Å –ø–µ—Ä–µ–¥ –Ω–∞–º–∏ —Å—Ç–æ–∏—Ç –ø—Ä–æ–±–ª–µ–º–∞: –º—ã –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫, –∞ –Ω–∞–º –Ω—É–∂–Ω—ã —Ç–µ–Ω–∑–æ—Ä—ã (–º–∞—Ç—Ä–∏—Ü—ã) —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤, –∫ —Ç–æ–º—É –∂–µ –Ω–∞–º –Ω—É–∂–Ω–æ –∑–∞–ø–∞–¥–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –≤—Å–µ —Å–ª–æ–∂–∏—Ç—å –≤ —Ç–æ—Ä—á–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É. –ú—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏:\n",
    "- –î–æ—Å—Ç–∞—Ç—å –∏–∑ `train/valid/test_dataset` –¥–∞–Ω–Ω—ã–µ –∏ –Ω–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π `Dataset`, –≥–¥–µ –≤–Ω—É—Ç—Ä–∏ –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, —Ç–æ–∫–µ–Ω—ã –±—É–¥—É—Ç –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å—Å—è –≤ –∏–Ω–¥–µ–∫—Å—ã –∏ –∑–∞—Ç–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç –ø–∞–¥–∏—Ç—å—Å—è –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "- –°–¥–µ–ª–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –Ω–∞—à–∏ –±–∞—Ç—á–∏. –û–Ω–∞ –≤—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤ `DataLoader(collate_fn=<–í–ê–®–ê_–§–£–ù–ö–¶–ò–Ø>)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b0f2e",
   "metadata": {},
   "source": [
    "## –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Å–≤–æ–π `Dataset`\n",
    "–¢–æ –≤—ã –º–æ–∂–µ—Ç–µ –¥–æ—Å—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26771f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45615, 45615)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[\"text\"]), len(train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25200be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"',\n",
       " '\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c952b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"label\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68184652",
   "metadata": {},
   "source": [
    "## –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å `collate_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706dab4d",
   "metadata": {},
   "source": [
    "### –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –≤–æ–æ–±—â–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞\n",
    "–î–ª—è —ç—Ç–æ–≥–æ —Å–¥–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é `empty_collate`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –±–∞—Ç—á –∏ –æ—Ç–¥–∞–µ—Ç –µ–≥–æ, –Ω–∏—á–µ–≥–æ —Å –Ω–∏–º –Ω–µ –¥–µ–ª–∞—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7ce86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_collate(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f0fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=empty_collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=empty_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=empty_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "942cf78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '@user @user @user Right, well. Merlin, I\\'ll be checking on you um... tomorrow. Please have clothes on.\"',\n",
       "  'label': 1},\n",
       " {'text': \"@user You baaad mutha shut yo mouth, just heard the interview for your memoirs @user Sunday. You're like an Oracle, a shinning Star.\",\n",
       "  'label': 1},\n",
       " {'text': 'Re doing a bio and watching the Republicans debate while smashing some Pineapple sherbet. Not sure what to make of this evening lol',\n",
       "  'label': 1},\n",
       " {'text': 'Not half as ridiculous as the story of a state govt. spending hundred of millions on prayer warriors against Boko Haram! #Sunday #Church',\n",
       "  'label': 1}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca5081",
   "metadata": {},
   "source": [
    "## –§–æ—Ä–º–∞—Ç –±–∞—Ç—á–∞\n",
    "```python\n",
    "batch = [\n",
    "    {\n",
    "        \"text\": \"text1\",\n",
    "        \"label\": 0\n",
    "    }, \n",
    "    {\n",
    "        \"text\": \"text2\",\n",
    "        \"label\": 1\n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        \"text\": \"textn\",\n",
    "        \"label\": 1\n",
    "    }\n",
    "]\n",
    "```\n",
    "–¢–æ –µ—Å—Ç—å —Ç–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ `text` –∏ `label`.  \n",
    "\n",
    "–í—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∏–ª–∏ –∫–ª–∞—Å—Å —Å –º–µ—Ç–æ–¥–æ–º `collate`. –≠—Ç–æ—Ç —Å–ø–æ—Å–æ–± —Ä–µ—à–µ–Ω–∏—è –¥–æ–º–∞—à–∫–∏ –ø—Ä–µ–¥–æ–¥—á—Ç–∏—Ç–µ–ª—å–Ω–µ–π, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `collate` –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞.\n",
    "\n",
    "–ß—Ç–æ —è –ø—Ä–µ–¥–ª–∞–≥–∞—é:\n",
    "- –°–¥–µ–ª–∞–π—Ç–µ –∫–ª–∞—Å—Å `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b9ddcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, base_tokenizer, token2index, pad_token, unk_token, max_length):\n",
    "\n",
    "        print(\"Tokenizer initializing...\")\n",
    "        self._base_tokenizer = base_tokenizer  # –Ω–∞–ø—Ä–∏–º–µ—Ä ToktokTokenizer()\n",
    "\n",
    "        self.token2index = token2index  # —Å–ª–æ–≤–∞—Ä—å –∏–∑ load_embeddings()\n",
    "        \n",
    "        print(\"Setting up the padding token...\")\n",
    "        self.pad_token = pad_token\n",
    "        if not self.pad_token in token2index.keys():\n",
    "            token2index[self.pad_token] = len(token2index)\n",
    "        self.pad_index = self.token2index[self.pad_token]\n",
    "        \n",
    "        print(\"Setting up the unknown token...\")\n",
    "        self.unk_token = unk_token\n",
    "        if not self.unk_token in token2index.keys():\n",
    "            token2index[self.unk_token] = len(token2index)\n",
    "        self.unk_index = self.token2index[self.unk_token]\n",
    "        \n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Initialization finished\")\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
    "        \"\"\"\n",
    "        return self._base_tokenizer.tokenize(text) # returns text.split()*\n",
    "            \n",
    "    \n",
    "    def indexing(self, tokenized_text):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —ç—Ç–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        \"\"\"\n",
    "        index_list = []\n",
    "        for token in tokenized_text:\n",
    "                index_list.append(self.token2index[token] if token in token2index.keys() \\\n",
    "                    else self.token2index[\"<unk>\"])\n",
    "        return index_list\n",
    "        \n",
    "\n",
    "    def padding(self, tokens_indices):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–ª–∏–Ω—É tokens_indices —Ä–∞–≤–Ω–æ–π self.max_length\n",
    "        –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–±—Ä–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è unk'–∏\n",
    "        \"\"\"\n",
    "        while len(tokens_indices) != self.max_length:\n",
    "            tokens_indices.append(self.token2index[self.pad_token])\n",
    "        \n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å—Ç—Ä–æ–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º –≤ –≤–µ–∫—Ç–æ—Ä —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Å–ª–æ–≤ –Ω—É–∂–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ (self.max_length)\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        index_vector = []\n",
    "        for token in tokens:\n",
    "            if token in token2index.keys():\n",
    "                index_vector.append(token2index[token])\n",
    "        \n",
    "        self.padding(index_vector)\n",
    "\n",
    "        return index_vector\n",
    "        \n",
    "    def collate(self, batch):\n",
    "        \"\"\"\n",
    "        batch sample {\n",
    "            text: str\n",
    "            label: int\n",
    "        }\n",
    "        \"\"\"\n",
    "        tokenized_texts = list()\n",
    "        labels = list()\n",
    "        \n",
    "        for sample in batch:\n",
    "            labels.append(sample['label'])\n",
    "            tokens = self.tokenize(sample['text'])\n",
    "            indices = self.indexing(tokens)\n",
    "            self.padding(indices)\n",
    "            tokenized_texts.append(indices)\n",
    "\n",
    "            \n",
    "        tokenized_texts = torch.Tensor(tokenized_texts)  # –ø–µ—Ä–µ–≤–æ–¥ –≤ torch.Tensor\n",
    "        labels = torch.Tensor(labels)  # –ø–µ—Ä–µ–≤–æ–¥ –≤ torch.Tensor\n",
    "        \n",
    "        return tokenized_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e15a9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, num_tokens=100_000):\n",
    "    \"\"\"\n",
    "    {label: int, text: str}\n",
    "    \"\"\"\n",
    "\n",
    "    # token2index: Dict[str, int] = {\n",
    "    #     \"<pad>\": 0,\n",
    "    #     \"<unk>\": 1\n",
    "    # }\n",
    "    token2index: Dict[str, int] = {}\n",
    "    embeddings_matrix: np.array = []\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        vocab_size, emb_dim = file.readline().strip().split()\n",
    "        emb_dim = int(emb_dim)\n",
    "        vocab_size = int(vocab_size)\n",
    "\n",
    "        num_tokens = min(num_tokens, vocab_size)\n",
    "\n",
    "        progress_bar = tqdm(total=num_tokens, disable=False, desc='Reading embeddings file')\n",
    "\n",
    "        for line in file:\n",
    "            content = line.strip().split()\n",
    "\n",
    "            token = ' '.join(content[:-emb_dim]).lower()\n",
    "\n",
    "            if token in token2index:\n",
    "                continue\n",
    "\n",
    "            word_vector = np.array(list(map(float, content[-emb_dim:])))\n",
    "            token2index[token] = len(token2index)\n",
    "            embeddings_matrix.append(word_vector)\n",
    "\n",
    "            progress_bar.update()\n",
    "\n",
    "            if len(token2index) == num_tokens:\n",
    "                break\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        embeddings_matrix = np.stack(embeddings_matrix)\n",
    "    \n",
    "    assert(len(token2index) == embeddings_matrix.shape[0])\n",
    "    \n",
    "    return token2index, embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d31402ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:05<00:00, 19772.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer initializing...\n",
      "Setting up the padding token...\n",
      "Setting up the unknown token...\n",
      "Initialization finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Tokenizer at 0x7fcd03af3100>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(ToktokTokenizer(), load_embeddings(path=\"cc.ru.300.vec\")[0], '<pad>', '<unk>', 100)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddca07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ab7e6d",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–µ–¥ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞\n",
    "–°–æ–≤–µ—Ç—É—é, —á—Ç–æ–±—ã –≤ –∏—Ç–æ–≥–µ `Loader` –æ—Ç–¥–∞–≤–∞–ª –∫–æ—Ä—Ç–µ–∂ —Å –¥–≤—É–º—è —Ç–µ–Ω–∑–æ—Ä–∞–º–∏:\n",
    "- `torch.Tensor` —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `(batch_size, sequence_length)`\n",
    "- `torch.Tensor` —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–∞—Ä–≥–µ—Ç–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `(batch_size)`\n",
    "\n",
    "–¢–æ –µ—Å—Ç—å, —á—Ç–æ–±—ã –±—ã–ª–æ —Ç–∞–∫:\n",
    "```python\n",
    "for x, y in train_loader:\n",
    "    ...\n",
    "\n",
    ">> x\n",
    ">> tensor([[   37,  3889,   470,  ...,     0,     0,     0],\n",
    "           [ 1509,   581,   144,  ...,     0,     0,     0],\n",
    "           [ 1804,   893,  2457,  ...,     0,     0,     0],\n",
    "           ...,\n",
    "           [  170, 39526,  2102,  ...,     0,     0,     0],\n",
    "           [ 1217,   172, 28440,  ...,     0,     0,     0],\n",
    "           [   37,    56,   603,  ...,     0,     0,     0]])\n",
    "\n",
    ">> y\n",
    ">> tensor([1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1,\n",
    "           0, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
    "           1, 0, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 0, 1, 0, 2, 1, 2, 2, 1, 0, 0, 2, 2,\n",
    "           2, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 0, 2, 2,\n",
    "           2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 2, 1, 2, 1,\n",
    "           2, 1, 1, 2, 2, 1, 1, 2])\n",
    "\n",
    ">> x.shape\n",
    ">> torch.Size([128, 64])\n",
    "\n",
    ">> y.shape\n",
    ">> torch.Size([128])\n",
    "```\n",
    "–ü—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ –±–∞—Ç—á —Å–∞–π–∑ —Ä–∞–≤–µ–Ω 128, –∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–≤–Ω–∞ 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09e6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n",
    "#!gzip -d cc.ru.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41ed0aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:04<00:00, 20039.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer initializing...\n",
      "Setting up the padding token...\n",
      "Setting up the unknown token...\n",
      "Initialization finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(ToktokTokenizer(), load_embeddings(path=\"cc.ru.300.vec\")[0], '<pad>', '<unk>', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40268f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=tokenizer.collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=tokenizer.collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=tokenizer.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbcf8016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fcd01115ee0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d293b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+05, 1.5250e+04, 3.4200e+03, 8.5517e+04, 3.4200e+03, 6.9238e+04,\n",
      "         2.7742e+04, 1.4232e+04, 1.5026e+04, 1.0000e+05, 3.4000e+01, 1.0979e+04,\n",
      "         3.1358e+04, 1.0000e+05, 1.0000e+05, 2.5000e+01, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05],\n",
      "        [1.0000e+05, 1.0000e+05, 6.6500e+02, 1.0000e+05, 4.1896e+04, 2.5000e+01,\n",
      "         5.7400e+02, 8.4210e+03, 3.6074e+04, 3.1500e+03, 0.0000e+00, 1.5492e+04,\n",
      "         1.0000e+05, 7.0500e+02, 1.0000e+05, 2.5000e+01, 8.6957e+04, 1.0000e+05,\n",
      "         2.5000e+01, 1.0000e+05, 1.0000e+05, 3.1730e+03, 1.0000e+05, 5.7400e+02,\n",
      "         1.0000e+05, 1.0000e+05, 7.0500e+02, 1.0000e+05, 4.0290e+04, 2.3732e+04,\n",
      "         2.5000e+01, 9.0000e+00, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
      "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05]]) tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    break\n",
    "\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87752ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(isinstance(x, torch.Tensor))\n",
    "assert(len(x.size()) == 2)\n",
    "\n",
    "assert(isinstance(y, torch.Tensor))\n",
    "assert(len(y.size()) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc96104",
   "metadata": {},
   "source": [
    "# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è DAN\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤\n",
    "\n",
    "–®–∞–≥–∏:\n",
    "- –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ `Multilayer Perceptron`\n",
    "    - –ù—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–∞–º–æ–º—É\n",
    "\n",
    "### –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ –¥–æ–º–∞—à–∫–µ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ—Ç–∏:\n",
    "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å skip-connection (residual connection) –≤ –ª–∏–Ω–µ–π–Ω–æ–º —Å–ª–æ–µ\n",
    "- –ù–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π —Å–ª–æ–π, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç (–ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–µ–≤ –Ω–∏–∂–µ –Ω–∞–ø—É—Ç–∞–Ω, —Ç–∞–∫ —á—Ç–æ —Å–∞–º–∏ –ø–æ–¥—É–º–∞–π—Ç–µ –≤ –∫–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ —Å—Ç–æ–∏—Ç —Ä–∞—Å–ø–æ–ª–æ–∂–∏—Ç—å —ç—Ç–∏ —Å–ª–æ–∏) :\n",
    "  - `Dropout`\n",
    "  - `BatchNorm` / `LayerNorm`\n",
    "  - `Residual`, –µ—Å–ª–∏ –≤—ã –Ω–µ –º–µ–Ω—è–µ—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "  - –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "  - –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
    "\n",
    "### –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è:\n",
    "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `transformers`\n",
    "- –°–¥–µ–ª–∞—Ç—å —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–∞–¥–æ–≤\n",
    "  - –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–¥—ã, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –µ–¥–∏–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –±–∞—Ç—á–µ\n",
    "    - –¢–æ –µ—Å—Ç—å —É –Ω–∞—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤ –±–∞—Ç—á–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, 16 —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ—ç—Ç–æ–º—É –∫–æ –≤—Å–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º, —É –∫–æ—Ç–æ—Ä—ã—Ö –¥–ª–∏–Ω–∞ –Ω–∏–∂–µ –º—ã –¥–æ–±–∞–≤–ª—è–µ–º `16 - len(sequence)` –ø–∞–¥–æ–≤\n",
    "  - –¢–æ –µ—Å—Ç—å –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ç–∞–∫, —á—Ç–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –¥–ª–∏–Ω—ã –≤ –±–∞—Ç—á–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ\n",
    "    - –°—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–∞ `[1, 2, 3]` –±—É–¥–µ—Ç `2`. –°—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–∞ `[1, 2, 3, 0, 0]` –±—É–¥–µ—Ç `1.2`\n",
    "    - –ü–æ–ª—É—á–∞–µ—Ç—Å—è, —á—Ç–æ —É—Å—Ä–µ–¥–Ω—è—è —Å –ø–∞–¥–∞–º–∏ –º—ã –ø–æ–ª—É—á–∞–µ–º \"–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π\" –≤–µ–∫—Ç–æ—Ä\n",
    "  - –¢–æ –µ—Å—Ç—å –Ω–∞—à–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Ç–æ–≥–æ —Å–∫–æ–ª—å–∫–æ –ø–∞–¥–æ–≤ —É –Ω–∞—Å –µ—Å—Ç—å –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏\n",
    "  - –ö–æ–≥–¥–∞ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à—É —Å–µ—Ç–∫—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –º—ã –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å –≤ –Ω–µ–µ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É, –≥–¥–µ –ø–∞–¥–æ–≤ –Ω–µ –±—É–¥–µ—Ç\n",
    "    - –¢–æ –µ—Å—Ç—å –ø–æ–ª—É—á–∞–µ—Ç—Å—è –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å –Ω–µ –≤ —Ç–æ–π –∂–µ —Å—Ä–µ–¥–µ, –∫–∞–∫ –∏ –æ–±—É—á–∞–ª–∏\n",
    "      - –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞—à–∏ –≤—Ö–æ–¥—ã –º–µ–Ω—è—é—Ç—Å—è, –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–¥—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –¥—Ä—É–≥–æ–π\n",
    "    - –≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è `distribution shift`, —Ç–æ –µ—Å—Ç—å –∫–æ–≥–¥–∞ –º—ã —É—á–∏–º—Å—è –Ω–∞ –æ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞ –¥—Ä—É–≥–∏—Ö\n",
    "      - –≠—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–ª–æ—Ö–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –∏–Ω–æ–≥–¥–∞ —Ç–æ–ª—å–∫–æ —Ç–∞–∫ –º—ã –∏ –º–æ–∂–µ–º —É—á–∏—Ç—å—Å—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –Ω—É–∂–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞\n",
    "      - –≠—Ç–æ –ø–ª–æ—Ö–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –º—ã –≤–Ω–æ—Å–∏–º —ç—Ç–æ \"—Å–ª—É—á–∞–π–Ω–æ\", –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ–º, —Ç–æ –µ—Å—Ç—å —ç—Ç–æ —Å–≤–æ–µ–±—Ä–∞–∑–Ω—ã–π –±–∞–≥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123324f0",
   "metadata": {},
   "source": [
    "## –î–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "- –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫—É(–∫–∏) –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ—á–µ–º—É –æ–Ω–∞(–æ–Ω–∏)\n",
    "    - –û–±—ã—á–Ω–æ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏—è –∫–∞–∫–∏–µ –≤–µ—Å–∞ –±—Ä–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø–æ–º–æ–≥—É—Ç –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ —Ç–æ–º –≤—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –Ω–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, —Ö–æ—Ä–æ—à–æ –ª–∏ –º–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ –∏ —Ç–¥\n",
    "- –≠—Ç—É –¥–æ–º–∞—à–∫—É –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∏ –Ω–∞ `CPU`, –Ω–æ –Ω–∞ `GPU` –±—É–¥–µ—Ç —Å–∏–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ\n",
    "    - –í–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–æ–º–∞—à–∫–∞—Ö –º—ã –±—É–¥–µ–º —É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ `GPU`\n",
    "    - –†–∞–Ω–æ –∏–ª–∏ –ø–æ–∑–¥–Ω–æ –≤–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —ç—Ç–æ—Ç [—Ç—É—Ç–æ—Ä–∏–∞–ª](https://www.youtube.com/watch?v=pgk1zGv5lU4)\n",
    "    - –í—ã –º–æ–∂–µ—Ç–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ `colab`, —ç—Ç–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ\n",
    "\n",
    "## –î–æ —ç–ø–æ—Ö–∏\n",
    "- –°–¥–µ–ª–∞–π—Ç–µ —Å–ø–∏—Å–∫–∏/—Å–ª–æ–≤–∞—Ä–∏/–¥—Ä—É–≥–æ–µ, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫(–∏) –ø–æ –≤—Å–µ–π —ç–ø–æ—Ö–µ –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "\n",
    "## –í–æ –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ [`tqdm`](https://github.com/tqdm/tqdm) –∫–∞–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –∫–∞–∫ –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤–∞—à–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- –õ–æ–≥–∏—Ä—É–π—Ç–µ –ª–æ—Å—Å\n",
    "- –õ–æ–≥–∏—Ä—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫—É(–∫–∏) –ø–æ –±–∞—Ç—á—É\n",
    "- –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫(–∏) –Ω–∞ –≤—Å—é —ç–ø–æ—Ö—É –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "\n",
    "## –ü–æ—Å–ª–µ —ç–ø–æ—Ö–∏\n",
    "- –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫(–∏) –Ω–∞ –≤—Å—é —ç–ø–æ—Ö—É –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "\n",
    "## –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "- –ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏:\n",
    "    - [Confusion Matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix)\n",
    "    - [–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—Ç–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∫–æ–≥–æ-—Ç–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã –≤—ã–±–∏—Ä–∞–µ–º —Ç–∞–∫–æ–π –∫–ª–∞—Å—Å –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–∞–∫–∞—è-—Ç–æ) –Ω–∞ —Ç—Ä–µ–π–Ω–µ/—Ç–µ—Å—Ç–µ/–≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        - –ï—Å–ª–∏ –∫–ª–∞—Å—Å –±—ã–ª –≤—ã–±—Ä–∞–Ω –≤–µ—Ä–Ω–æ –∏ –µ—Å–ª–∏ –±—ã–ª–∞ –æ—à–∏–±–∫–∞\n",
    "- –ü–æ–¥—É–º–∞–π—Ç–µ —á—Ç–æ –µ—â–µ –≤–∞–º –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Ç–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã: \n",
    "    - –ß—Ç–æ –≤ –º–æ–¥–µ–ª–µ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å?\n",
    "    - –í—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –º–æ–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏?\n",
    "    - –í—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π?\n",
    "    - –ù–µ –ø–µ—Ä–µ–æ–±—É—á–∏–ª—Å—è –ª–∏ —è?\n",
    "    - –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —è –ø–æ—Å–º–æ—Ç—Ä–µ–ª –Ω–∞ –¥–∞–Ω–Ω—ã–µ?\n",
    "    - –ù—É–∂–Ω–æ –ª–∏ –º–Ω–µ —É–ª—É—á—à–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö?\n",
    "    - –ù—É–∂–Ω–æ –ª–∏ –ø–æ–º–µ–Ω—è—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏?\n",
    "    - –ù–µ—Ç –ª–∏ —É –º–µ–Ω—è –±–∞–≥–æ–≤ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏?\n",
    "    - –ö–∞–∫–∏–µ —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ —É –º–æ–µ–π –º–æ–¥–µ–ª–∏?\n",
    "    - –ö–∞–∫ —è –º–æ–≥—É –∏—Ö –∏—Å–ø—Ä–∞–≤–∏—Ç—å?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a067d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:05<00:00, 18840.72it/s]\n"
     ]
    }
   ],
   "source": [
    "token2index, embeddings = load_embeddings(\"cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da42260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAverageNetwork(nn.Module):\n",
    "    # out_dim = 3, –ø–æ—Ç–æ–º—É —á—Ç–æ 3 –∫–ª–∞—Å—Å–∞: –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
    "    def __init__(self, emb_dim, embeddings = torch.Tensor(embeddings), out_dim = 3, token2index = token2index) -> None:\n",
    "        super().__init__()\n",
    "        self.token2index = token2index\n",
    "        self.emb_dim = emb_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.vocab_size = len(token2index)\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_dim).from_pretrained(embeddings)\n",
    "        \n",
    "        # modules\n",
    "        # - `Dropout`\n",
    "        # - `BatchNorm` / `LayerNorm`\n",
    "        # - `Residual`, –µ—Å–ª–∏ –≤—ã –Ω–µ –º–µ–Ω—è–µ—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "        # - –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "        # - –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
    "        self.linear1 = torch.nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.linear2 = torch.nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.norm = torch.nn.BatchNorm1d(self.emb_dim)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.activate = torch.nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = self.norm(x)\n",
    "        x = self.relu(self.linear1(norm))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x)) + norm\n",
    "        x = self.activate(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7966f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepAverageNetwork(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b4a2161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepAverageNetwork(\n",
       "  (embeddings): Embedding(100000, 300)\n",
       "  (linear1): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (activate): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efad1d1",
   "metadata": {},
   "source": [
    "## –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34c552fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628847fe",
   "metadata": {},
   "source": [
    "## –°–¥–µ–ª–∞–π—Ç–µ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b634b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 0, best test loss: 100000.0 on epoch -1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m tr_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m i, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     14\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Deps/miniconda3/envs/cv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Deps/miniconda3/envs/cv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Deps/miniconda3/envs/cv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn [11], line 84\u001b[0m, in \u001b[0;36mTokenizer.collate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     labels\u001b[39m.\u001b[39mappend(sample[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     83\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(sample[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 84\u001b[0m     indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindexing(tokens)\n\u001b[1;32m     85\u001b[0m     tokenized_texts\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding(indices))\n\u001b[1;32m     88\u001b[0m tokenized_texts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(tokenized_texts)  \u001b[39m# –ø–µ—Ä–µ–≤–æ–¥ –≤ torch.Tensor\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [11], line 39\u001b[0m, in \u001b[0;36mTokenizer.indexing\u001b[0;34m(self, tokenized_text)\u001b[0m\n\u001b[1;32m     37\u001b[0m index_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenized_text:\n\u001b[0;32m---> 39\u001b[0m     index_list\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken2index[token])\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m index_list\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MT'"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5  # –ó–∞–¥–∞–π—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "# –î–æ–±–∞–≤–∏—Ç—å .to(device) —á—Ç–æ–±—ã —É—á–∏—Ç—å –Ω–∞ –≥–ø—É?\n",
    "best_epoch = -1\n",
    "best_test_loss = 1e5\n",
    "f1_score = -1e5\n",
    "for n_epoch in range(NUM_EPOCHS):\n",
    "    # TRAIN\n",
    "    print(f\"Running epoch {n_epoch}, best test loss: {best_test_loss} on epoch {best_epoch}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y, y_pred)\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    tr_loss /= step\n",
    "    \n",
    "    # TEST\n",
    "    step = 0\n",
    "    te_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            step += 1\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y, y_pred)\n",
    "            te_loss += loss.item()\n",
    "        te_loss /= step\n",
    "\n",
    "    if te_loss < best_te_loss:\n",
    "        best_te_loss = te_loss\n",
    "        best_ep = n_epoch\n",
    "        # torch.save(model.state_dict(), f\"best_model.pt\")\n",
    "    print(f\"epoch {n_epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
    "\n",
    "\n",
    "    \n",
    "# VALIDATE\n",
    "print(\"Validating...\")\n",
    "step = 0\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (x, y) in enumerate(valid_loader):\n",
    "        step += 1\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y, y_pred)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= step\n",
    "print(f\"val_loss = {val_loss}\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87583e79",
   "metadata": {},
   "source": [
    "# –í—ã–≤–æ–¥—ã\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ –Ω–µ–±–æ–ª—å—à–æ–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ. –ß—Ç–æ —É–¥–∞–ª–æ—Å—å, –≤ —á–µ–º –Ω–µ —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad7f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "c881db0e76e4d07227a54605e4579ac40ea89326ca18881e5cf3019b0657001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
