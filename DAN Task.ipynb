{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f9c64fb",
        "outputId": "1176504a-2bb9-4d9f-9b8a-a7128e1f6647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from sklearn.metrics import f1_score\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "id": "7f9c64fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102d7074"
      },
      "source": [
        "# Deep Average Network –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞ "
      ],
      "id": "102d7074"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d972fe"
      },
      "source": [
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–∫–µ –º—ã –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–∏—Ç—ã –Ω–∞ 3 —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.  \n",
        "–í—ã –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤, —Ç–∞–∫ —á—Ç–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å [—Ç—É—Ç–æ—Ä–∏–∞–ª –ø–æ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é](https://github.com/BobaZooba/DeepNLP/blob/master/Tutorials/Word%20vectors%20%26%20Data%20Loading.ipynb).\n",
        "\n",
        "–ù–∞—à–∏ –∫–ª–∞—Å—Å—ã:  \n",
        "\n",
        "–ò–Ω–¥–µ–∫—Å | Sentiment  \n",
        "-- | --  \n",
        "0 | negative  \n",
        "1 | neutral  \n",
        "2 | positive  "
      ],
      "id": "46d972fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55010212"
      },
      "source": [
        "–í–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–∞–∫—É—é –º–æ–¥–µ–ª—å:\n",
        "![–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ DAN](https://www.researchgate.net/profile/Shervin-Minaee/publication/340523298/figure/fig1/AS:878252264550411@1586403065555/The-architecture-of-the-Deep-Average-Network-DAN-10.ppm)\n",
        "\n",
        "–ß—Ç–æ –æ–Ω–∞ –∏–∑ —Å–µ–±—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç:\n",
        "- –ú—ã –ø–æ–¥–∞–µ–º –≤ –Ω–µ–µ –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤\n",
        "- –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "- –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ `Multilayer Perceptron`\n",
        "\n",
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–∫–µ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:\n",
        "- –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç—ã –≤ –º–∞—Ç—Ä–∏—Ü—ã —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å\n",
        "- –û–±—É—á–∏—Ç—å –µ–µ\n",
        "- –ü–æ–Ω—è—Ç—å —Ö–æ—Ä–æ—à–æ –ª–∏ –≤—ã —ç—Ç–æ —Å–¥–µ–ª–∞–ª–∏\n",
        "\n",
        "–≠—Ç–æ –æ—á–µ–Ω—å –≤–∞–∂–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏. –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–∞ —Ä–∞–±–æ—Ç–µ —Å–æ–≤–µ—Ç—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–∫—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –±–µ–π–∑–ª–∞–π–Ω. –ò –≤ –∫–∞—á–µ—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤ –≤–∑—è—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç –±–µ—Ä—Ç–∞/—Ä–æ–±–µ—Ä—Ç—ã/—Ç–¥."
      ],
      "id": "55010212"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65faf753"
      },
      "source": [
        "## ü§ó Datasets\n",
        "–í —ç—Ç–æ–º —Ç—É—Ç–æ—Ä–∏–∞–ª–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ [datasets](https://github.com/huggingface/datasets). –ú—ã –≤—Ä—è–¥ –ª–∏ –µ—â–µ –±—É–¥–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —ç—Ç–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π, —Ç–∞–∫ –∫–∞–∫ –Ω–∞–º –±—É–¥–µ—Ç –≤–∞–∂–Ω–æ —Å–∞–º–∏–º –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –≤–æ-–≤—Ç–æ—Ä—ã—Ö, –∑–¥–µ—Å—å –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ–ø–ª–æ—Ö–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏. [–ó–¥–µ—Å—å](https://huggingface.co/datasets) –≤—ã —Å–º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–≥–¥–∞-–Ω–∏–±—É–¥—å –æ–Ω–∏ –≤–∞–º –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è."
      ],
      "id": "65faf753"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5927f02a"
      },
      "source": [
        "## –ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤\n",
        "–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ –∑–∞–≥—Ä—É–∑–∫–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤ –∏ `np.array`\n",
        "–§–æ—Ä–º–∞—Ç —Å–ª–æ–≤–∞—Ä—è:\n",
        "```python\n",
        "{\n",
        "    'aabra': 0,\n",
        "    ...,\n",
        "    'mom': 6546,\n",
        "    ...\n",
        "    'xyz': 100355\n",
        "}\n",
        "```\n",
        "–§–æ—Ä–º–∞—Ç –º–∞—Ç—Ä–∏—Ü—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:\n",
        "```python\n",
        "array([[0.44442278, 0.28644582, 0.04357426, ..., 0.9425766 , 0.02024289,\n",
        "        0.88456545],\n",
        "       [0.77599317, 0.35188237, 0.54801261, ..., 0.91134102, 0.88599103,\n",
        "        0.88068835],\n",
        "       [0.68071886, 0.29352313, 0.95952505, ..., 0.19127958, 0.97723054,\n",
        "        0.36294011],\n",
        "       ...,\n",
        "       [0.03589378, 0.85429694, 0.33437761, ..., 0.39784873, 0.80368014,\n",
        "        0.76368042],\n",
        "       [0.01498725, 0.78155695, 0.80372969, ..., 0.82051826, 0.42314861,\n",
        "        0.18655465],\n",
        "       [0.69263802, 0.82090775, 0.27150426, ..., 0.86582747, 0.40896573,\n",
        "        0.33423976]])\n",
        "```\n",
        "\n",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –º–∞—Ç—Ä–∏—Ü–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ä–∞–∑–º–µ—Ä–æ–º —Å–ª–æ–≤–∞—Ä—è, —Ç–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥. –ü–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É `num_tokens` –¥–æ–ª–∂–Ω–æ –±—Ä–∞—Ç—å –Ω–µ –±–æ–ª–µ–µ —É–∫–∞–∑–∞–Ω–æ –≤ —ç—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä—å –∏ –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."
      ],
      "id": "5927f02a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c46b2b68"
      },
      "source": [
        "## –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "–ú—ã —Å—Ä–∞–∑—É –ø–æ–ª—É—á–∏–º `torch.utils.data.Dataset`, –∫–æ—Ç–æ—Ä—ã–π —Å–º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –≤ `torch.utils.data.DataLoader`"
      ],
      "id": "c46b2b68"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e54fdaa8",
        "outputId": "94b709bb-9299-4563-d5aa-d266a13d6388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"tweet_eval\"\n",
        "dataset_name = \"sentiment\"\n",
        "\n",
        "train_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"train\")\n",
        "valid_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"validation\")\n",
        "test_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"test\")"
      ],
      "id": "e54fdaa8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad4a0650"
      },
      "source": [
        "## `torch.utils.data.DataLoader`"
      ],
      "id": "ad4a0650"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dc742027"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
      ],
      "id": "dc742027"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9012ae5"
      },
      "source": [
        "## –ü–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –æ—Ç–¥–∞–µ—Ç –Ω–∞–º `Loader`\n",
        "–≠—Ç–æ –±–∞—Ç—á —Ñ–æ—Ä–º–∞—Ç–∞:\n",
        "```python\n",
        "batch = {\n",
        "    \"text\": [\n",
        "        \"text1\",\n",
        "        \"text2\",\n",
        "        ...,\n",
        "        \"textn\"\n",
        "    ],\n",
        "    \"label\": tensor([\n",
        "        1,\n",
        "        1,\n",
        "        ...,\n",
        "        0\n",
        "    ])\n",
        "}\n",
        "```\n",
        "–¢–æ –µ—Å—Ç—å —É –Ω–∞—Å –µ—Å—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å –¥–≤—É–º—è –∫–ª—é—á–∞–º–∏ `text` –∏ `label`, –≥–¥–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è n –ø—Ä–∏–º–µ—Ä–æ–≤. –¢–æ –µ—Å—Ç—å –¥–ª—è 5-–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ —Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ `batch[\"text\"][5]`, –∞ –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞ –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ `batch[\"label\"][5]`."
      ],
      "id": "e9012ae5"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bf6b50",
        "outputId": "5ee09316-24b0-425c-d3d0-3faa8d1f2942"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['\"when the workers at Dunkin say \"\"see you tomorrow\"\" @user @user',\n",
              "  'Captain Dubs! David Wright gives the Mets back the lead with an RBI single to make it 6-5 Mets in the 7th inning'],\n",
              " 'label': tensor([1, 2])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    break\n",
        "\n",
        "batch"
      ],
      "id": "49bf6b50"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b9f0e6"
      },
      "source": [
        "## Collate\n",
        "–°–µ–π—á–∞—Å –ø–µ—Ä–µ–¥ –Ω–∞–º–∏ —Å—Ç–æ–∏—Ç –ø—Ä–æ–±–ª–µ–º–∞: –º—ã –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫, –∞ –Ω–∞–º –Ω—É–∂–Ω—ã —Ç–µ–Ω–∑–æ—Ä—ã (–º–∞—Ç—Ä–∏—Ü—ã) —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤, –∫ —Ç–æ–º—É –∂–µ –Ω–∞–º –Ω—É–∂–Ω–æ –∑–∞–ø–∞–¥–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –≤—Å–µ —Å–ª–æ–∂–∏—Ç—å –≤ —Ç–æ—Ä—á–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É. –ú—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏:\n",
        "- –î–æ—Å—Ç–∞—Ç—å –∏–∑ `train/valid/test_dataset` –¥–∞–Ω–Ω—ã–µ –∏ –Ω–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π `Dataset`, –≥–¥–µ –≤–Ω—É—Ç—Ä–∏ –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, —Ç–æ–∫–µ–Ω—ã –±—É–¥—É—Ç –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å—Å—è –≤ –∏–Ω–¥–µ–∫—Å—ã –∏ –∑–∞—Ç–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç –ø–∞–¥–∏—Ç—å—Å—è –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã\n",
        "- –°–¥–µ–ª–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –Ω–∞—à–∏ –±–∞—Ç—á–∏. –û–Ω–∞ –≤—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤ `DataLoader(collate_fn=<–í–ê–®–ê_–§–£–ù–ö–¶–ò–Ø>)`"
      ],
      "id": "b0b9f0e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03b0f2e"
      },
      "source": [
        "## –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Å–≤–æ–π `Dataset`\n",
        "–¢–æ –≤—ã –º–æ–∂–µ—Ç–µ –¥–æ—Å—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º."
      ],
      "id": "d03b0f2e"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26771f88",
        "outputId": "579e9818-d618-4234-c02e-f790ac784885"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45615, 45615)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(train_dataset[\"text\"]), len(train_dataset[\"label\"])"
      ],
      "id": "26771f88"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f25200be",
        "outputId": "1d773c73-958b-4cf2-8b37-4e7ccdb41de1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"',\n",
              " '\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_dataset[\"text\"][:2]"
      ],
      "id": "f25200be"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4c952b6",
        "outputId": "88f00083-bb28-475f-d24a-7727ab930033"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_dataset[\"label\"][:2]"
      ],
      "id": "d4c952b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68184652"
      },
      "source": [
        "## –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å `collate_fn`"
      ],
      "id": "68184652"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706dab4d"
      },
      "source": [
        "### –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –≤–æ–æ–±—â–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞\n",
        "–î–ª—è —ç—Ç–æ–≥–æ —Å–¥–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é `empty_collate`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –±–∞—Ç—á –∏ –æ—Ç–¥–∞–µ—Ç –µ–≥–æ, –Ω–∏—á–µ–≥–æ —Å –Ω–∏–º –Ω–µ –¥–µ–ª–∞—è"
      ],
      "id": "706dab4d"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3b7ce86b"
      },
      "outputs": [],
      "source": [
        "def empty_collate(batch):\n",
        "    return batch"
      ],
      "id": "3b7ce86b"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "26f0fe92"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=empty_collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=empty_collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=empty_collate)"
      ],
      "id": "26f0fe92"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "942cf78e",
        "outputId": "fa1e77fa-916f-4833-f1d4-c67a8569c072"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '\"Is Carly Fiorina Better Off Being Left Out?: Despite her rise in the polls, the GOP contender still may not ma...',\n",
              "  'label': 1},\n",
              " {'text': \"Mayweather ends feud with Rousey: 'I wish her nothing but the best'\",\n",
              "  'label': 1},\n",
              " {'text': '\"Stylistically this is a horrible match up for Rousey, look at the stats. I won\\'t be surprised if Rousey takes the L in January #UFC195\"',\n",
              "  'label': 0},\n",
              " {'text': \"ICYMI: CNN's criteria for the next Republican debate may end up leaving Carly Fiorina out.\",\n",
              "  'label': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    break\n",
        "\n",
        "batch"
      ],
      "id": "942cf78e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ca5081"
      },
      "source": [
        "## –§–æ—Ä–º–∞—Ç –±–∞—Ç—á–∞\n",
        "```python\n",
        "batch = [\n",
        "    {\n",
        "        \"text\": \"text1\",\n",
        "        \"label\": 0\n",
        "    }, \n",
        "    {\n",
        "        \"text\": \"text2\",\n",
        "        \"label\": 1\n",
        "    },\n",
        "    ...,\n",
        "    {\n",
        "        \"text\": \"textn\",\n",
        "        \"label\": 1\n",
        "    }\n",
        "]\n",
        "```\n",
        "–¢–æ –µ—Å—Ç—å —Ç–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ `text` –∏ `label`.  \n",
        "\n",
        "–í—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∏–ª–∏ –∫–ª–∞—Å—Å —Å –º–µ—Ç–æ–¥–æ–º `collate`. –≠—Ç–æ—Ç —Å–ø–æ—Å–æ–± —Ä–µ—à–µ–Ω–∏—è –¥–æ–º–∞—à–∫–∏ –ø—Ä–µ–¥–æ–¥—á—Ç–∏—Ç–µ–ª—å–Ω–µ–π, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `collate` –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞.\n",
        "\n",
        "–ß—Ç–æ —è –ø—Ä–µ–¥–ª–∞–≥–∞—é:\n",
        "- –°–¥–µ–ª–∞–π—Ç–µ –∫–ª–∞—Å—Å `Tokenizer`"
      ],
      "id": "91ca5081"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3b9ddcca"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    \n",
        "    def __init__(self, base_tokenizer, token2index, pad_token, unk_token, max_length):\n",
        "\n",
        "        print(\"Tokenizer initializing...\")\n",
        "        self._base_tokenizer = base_tokenizer  # –Ω–∞–ø—Ä–∏–º–µ—Ä ToktokTokenizer()\n",
        "\n",
        "        self.token2index = token2index  # —Å–ª–æ–≤–∞—Ä—å –∏–∑ load_embeddings()\n",
        "        \n",
        "        print(\"Setting up the padding token...\")\n",
        "        self.pad_token = pad_token\n",
        "        if not self.pad_token in token2index.keys():\n",
        "            token2index[self.pad_token] = len(token2index)\n",
        "        self.pad_index = self.token2index[self.pad_token]\n",
        "        \n",
        "        print(\"Setting up the unknown token...\")\n",
        "        self.unk_token = unk_token\n",
        "        if not self.unk_token in token2index.keys():\n",
        "            token2index[self.unk_token] = len(token2index)\n",
        "        self.unk_index = self.token2index[self.unk_token]\n",
        "        \n",
        "        self.max_length = max_length\n",
        "\n",
        "        print(\"Initialization finished\")\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
        "        \"\"\"\n",
        "        return self._base_tokenizer.tokenize(text) # returns text.split()*\n",
        "            \n",
        "    \n",
        "    def indexing(self, tokenized_text):\n",
        "        \"\"\"\n",
        "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —ç—Ç–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
        "        \"\"\"\n",
        "        index_list = []\n",
        "        for token in tokenized_text:\n",
        "                index_list.append(self.token2index[token] if token in token2index.keys() \\\n",
        "                    else self.token2index[\"<unk>\"])\n",
        "        return index_list\n",
        "        \n",
        "\n",
        "    def padding(self, tokens_indices):\n",
        "        \"\"\"\n",
        "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–ª–∏–Ω—É tokens_indices —Ä–∞–≤–Ω–æ–π self.max_length\n",
        "        –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–±—Ä–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è unk'–∏\n",
        "        \"\"\"\n",
        "        while len(tokens_indices) != self.max_length:\n",
        "            tokens_indices.append(self.token2index[self.pad_token])\n",
        "        \n",
        "    \n",
        "    def __call__(self, text):\n",
        "        \"\"\"\n",
        "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å—Ç—Ä–æ–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º –≤ –≤–µ–∫—Ç–æ—Ä —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Å–ª–æ–≤ –Ω—É–∂–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ (self.max_length)\n",
        "        \"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        index_vector = []\n",
        "        for token in tokens:\n",
        "            if token in token2index.keys():\n",
        "                index_vector.append(token2index[token])\n",
        "        \n",
        "        self.padding(index_vector)\n",
        "\n",
        "        return index_vector\n",
        "        \n",
        "    def collate(self, batch):\n",
        "        \"\"\"\n",
        "        batch sample {\n",
        "            text: str\n",
        "            label: int\n",
        "        }\n",
        "        \"\"\"\n",
        "        tokenized_texts = list()\n",
        "        labels = list()\n",
        "        \n",
        "        for sample in batch:\n",
        "            labels.append(sample['label'])\n",
        "            tokens = self.tokenize(sample['text'])\n",
        "            indices = self.indexing(tokens)\n",
        "            self.padding(indices)\n",
        "            tokenized_texts.append(indices)\n",
        "\n",
        "            \n",
        "        tokenized_texts = torch.Tensor(tokenized_texts)  # –ø–µ—Ä–µ–≤–æ–¥ –≤ torch.Tensor\n",
        "        labels = torch.Tensor(labels)  # –ø–µ—Ä–µ–≤–æ–¥ –≤ torch.Tensor\n",
        "        \n",
        "        return tokenized_texts, labels"
      ],
      "id": "3b9ddcca"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e15a9f7c"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(path, num_tokens=100_000):\n",
        "    \"\"\"\n",
        "    {label: int, text: str}\n",
        "    \"\"\"\n",
        "\n",
        "    # token2index: Dict[str, int] = {\n",
        "    #     \"<pad>\": 0,\n",
        "    #     \"<unk>\": 1\n",
        "    # }\n",
        "    token2index: Dict[str, int] = {}\n",
        "    embeddings_matrix: np.array = []\n",
        "\n",
        "    with open(path, 'r') as file:\n",
        "        vocab_size, emb_dim = file.readline().strip().split()\n",
        "        emb_dim = int(emb_dim)\n",
        "        vocab_size = int(vocab_size)\n",
        "\n",
        "        num_tokens = min(num_tokens, vocab_size)\n",
        "\n",
        "        progress_bar = tqdm(total=num_tokens, disable=False, desc='Reading embeddings file')\n",
        "\n",
        "        for line in file:\n",
        "            content = line.strip().split()\n",
        "\n",
        "            token = ' '.join(content[:-emb_dim]).lower()\n",
        "\n",
        "            if token in token2index:\n",
        "                continue\n",
        "\n",
        "            word_vector = np.array(list(map(float, content[-emb_dim:])))\n",
        "            token2index[token] = len(token2index)\n",
        "            embeddings_matrix.append(word_vector)\n",
        "\n",
        "            progress_bar.update()\n",
        "\n",
        "            if len(token2index) == num_tokens:\n",
        "                break\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        embeddings_matrix = np.stack(embeddings_matrix)\n",
        "    \n",
        "    assert(len(token2index) == embeddings_matrix.shape[0])\n",
        "    \n",
        "    return token2index, embeddings_matrix"
      ],
      "id": "e15a9f7c"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d31402ba",
        "outputId": "026c8950-203b-47ab-a446-be6ab273a520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading embeddings file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:15<00:00, 6301.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer initializing...\n",
            "Setting up the padding token...\n",
            "Setting up the unknown token...\n",
            "Initialization finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Tokenizer at 0x7fed3ad498e0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokenizer = Tokenizer(ToktokTokenizer(), load_embeddings(path=\"cc.ru.300.vec\")[0], '<pad>', '<unk>', 100)\n",
        "tokenizer"
      ],
      "id": "d31402ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ab7e6d"
      },
      "source": [
        "## –ü–µ—Ä–µ–¥ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞\n",
        "–°–æ–≤–µ—Ç—É—é, —á—Ç–æ–±—ã –≤ –∏—Ç–æ–≥–µ `Loader` –æ—Ç–¥–∞–≤–∞–ª –∫–æ—Ä—Ç–µ–∂ —Å –¥–≤—É–º—è —Ç–µ–Ω–∑–æ—Ä–∞–º–∏:\n",
        "- `torch.Tensor` —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `(batch_size, sequence_length)`\n",
        "- `torch.Tensor` —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–∞—Ä–≥–µ—Ç–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `(batch_size)`\n",
        "\n",
        "–¢–æ –µ—Å—Ç—å, —á—Ç–æ–±—ã –±—ã–ª–æ —Ç–∞–∫:\n",
        "```python\n",
        "for x, y in train_loader:\n",
        "    ...\n",
        "\n",
        ">> x\n",
        ">> tensor([[   37,  3889,   470,  ...,     0,     0,     0],\n",
        "           [ 1509,   581,   144,  ...,     0,     0,     0],\n",
        "           [ 1804,   893,  2457,  ...,     0,     0,     0],\n",
        "           ...,\n",
        "           [  170, 39526,  2102,  ...,     0,     0,     0],\n",
        "           [ 1217,   172, 28440,  ...,     0,     0,     0],\n",
        "           [   37,    56,   603,  ...,     0,     0,     0]])\n",
        "\n",
        ">> y\n",
        ">> tensor([1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1,\n",
        "           0, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
        "           1, 0, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 0, 1, 0, 2, 1, 2, 2, 1, 0, 0, 2, 2,\n",
        "           2, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 0, 2, 2,\n",
        "           2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 2, 1, 2, 1,\n",
        "           2, 1, 1, 2, 2, 1, 1, 2])\n",
        "\n",
        ">> x.shape\n",
        ">> torch.Size([128, 64])\n",
        "\n",
        ">> y.shape\n",
        ">> torch.Size([128])\n",
        "```\n",
        "–ü—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ –±–∞—Ç—á —Å–∞–π–∑ —Ä–∞–≤–µ–Ω 128, –∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–≤–Ω–∞ 64."
      ],
      "id": "11ab7e6d"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b09e6a9f",
        "outputId": "0e3aa779-1d91-435d-e61c-5780580b9688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-02 12:05:25--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1306357571 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‚Äòcc.ru.300.vec.gz‚Äô\n",
            "\n",
            "cc.ru.300.vec.gz    100%[===================>]   1.22G  42.4MB/s    in 30s     \n",
            "\n",
            "2022-12-02 12:05:56 (41.7 MB/s) - ‚Äòcc.ru.300.vec.gz‚Äô saved [1306357571/1306357571]\n",
            "\n",
            "gzip: cc.ru.300.vec already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n",
        "!gzip -d cc.ru.300.vec.gz"
      ],
      "id": "b09e6a9f"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ed0aa1",
        "outputId": "015b8406-0949-4cad-a085-0937fac9a8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading embeddings file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:09<00:00, 10243.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer initializing...\n",
            "Setting up the padding token...\n",
            "Setting up the unknown token...\n",
            "Initialization finished\n"
          ]
        }
      ],
      "source": [
        "token2index, embeddings = load_embeddings(path=\"cc.ru.300.vec\")\n",
        "tokenizer = Tokenizer(ToktokTokenizer(), token2index, '<pad>', '<unk>', 300)"
      ],
      "id": "41ed0aa1"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "40268f8c"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=tokenizer.collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=tokenizer.collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=tokenizer.collate)"
      ],
      "id": "40268f8c"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbcf8016",
        "outputId": "bc1ea0ab-ca0b-44aa-8a24-c29e3d035502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fed3ad49910>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "train_loader"
      ],
      "id": "fbcf8016"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d293b759",
        "outputId": "ad86aa47-7c1e-4522-e647-cea3efdb0b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.0000e+00, 1.0000e+05, 4.0290e+04, 2.7900e+02, 3.4200e+03, 1.6332e+04,\n",
            "         3.4884e+04, 0.0000e+00, 1.0000e+05, 8.2780e+03, 6.6500e+02, 4.4892e+04,\n",
            "         2.1720e+03, 9.1720e+03, 2.7900e+02, 4.5560e+03, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 4.9622e+04, 6.6500e+02, 1.1581e+04, 7.8370e+03, 7.2300e+02,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 6.6500e+02, 3.5000e+01, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05],\n",
            "        [1.0000e+05, 1.0000e+05, 2.2819e+04, 3.4200e+03, 3.7000e+01, 1.2300e+03,\n",
            "         6.6500e+02, 6.6930e+04, 2.7900e+02, 1.0000e+05, 1.0000e+05, 0.0000e+00,\n",
            "         2.5007e+04, 4.3492e+04, 2.0000e+01, 1.0000e+05, 0.0000e+00, 1.0000e+05,\n",
            "         3.7000e+01, 2.5000e+03, 6.6500e+02, 6.6930e+04, 2.7900e+02, 1.0000e+05,\n",
            "         5.7400e+02, 1.0000e+05, 3.7000e+01, 2.7432e+04, 3.3819e+04, 8.5846e+04,\n",
            "         6.6500e+02, 1.0000e+05, 2.1720e+03, 3.5053e+04, 1.0000e+00, 9.0000e+00,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05,\n",
            "         1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05, 1.0000e+05]]) tensor([2., 2.])\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_loader:\n",
        "    break\n",
        "\n",
        "print(x, y)"
      ],
      "id": "d293b759"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "87752ad7"
      },
      "outputs": [],
      "source": [
        "assert(isinstance(x, torch.Tensor))\n",
        "assert(len(x.size()) == 2)\n",
        "\n",
        "assert(isinstance(y, torch.Tensor))\n",
        "assert(len(y.size()) == 1)"
      ],
      "id": "87752ad7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbc96104"
      },
      "source": [
        "# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è DAN\n",
        "\n",
        "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤\n",
        "\n",
        "–®–∞–≥–∏:\n",
        "- –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "- –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ `Multilayer Perceptron`\n",
        "    - –ù—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–∞–º–æ–º—É\n",
        "\n",
        "### –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ –¥–æ–º–∞—à–∫–µ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ—Ç–∏:\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å skip-connection (residual connection) –≤ –ª–∏–Ω–µ–π–Ω–æ–º —Å–ª–æ–µ\n",
        "- –ù–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π —Å–ª–æ–π, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç (–ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–µ–≤ –Ω–∏–∂–µ –Ω–∞–ø—É—Ç–∞–Ω, —Ç–∞–∫ —á—Ç–æ —Å–∞–º–∏ –ø–æ–¥—É–º–∞–π—Ç–µ –≤ –∫–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ —Å—Ç–æ–∏—Ç —Ä–∞—Å–ø–æ–ª–æ–∂–∏—Ç—å —ç—Ç–∏ —Å–ª–æ–∏) :\n",
        "  - `Dropout`\n",
        "  - `BatchNorm` / `LayerNorm`\n",
        "  - `Residual`, –µ—Å–ª–∏ –≤—ã –Ω–µ –º–µ–Ω—è–µ—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
        "  - –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "  - –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
        "\n",
        "### –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è:\n",
        "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `transformers`\n",
        "- –°–¥–µ–ª–∞—Ç—å —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–∞–¥–æ–≤\n",
        "  - –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–¥—ã, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –µ–¥–∏–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –±–∞—Ç—á–µ\n",
        "    - –¢–æ –µ—Å—Ç—å —É –Ω–∞—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤ –±–∞—Ç—á–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, 16 —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ—ç—Ç–æ–º—É –∫–æ –≤—Å–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º, —É –∫–æ—Ç–æ—Ä—ã—Ö –¥–ª–∏–Ω–∞ –Ω–∏–∂–µ –º—ã –¥–æ–±–∞–≤–ª—è–µ–º `16 - len(sequence)` –ø–∞–¥–æ–≤\n",
        "  - –¢–æ –µ—Å—Ç—å –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ç–∞–∫, —á—Ç–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –¥–ª–∏–Ω—ã –≤ –±–∞—Ç—á–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ\n",
        "    - –°—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–∞ `[1, 2, 3]` –±—É–¥–µ—Ç `2`. –°—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–∞ `[1, 2, 3, 0, 0]` –±—É–¥–µ—Ç `1.2`\n",
        "    - –ü–æ–ª—É—á–∞–µ—Ç—Å—è, —á—Ç–æ —É—Å—Ä–µ–¥–Ω—è—è —Å –ø–∞–¥–∞–º–∏ –º—ã –ø–æ–ª—É—á–∞–µ–º \"–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π\" –≤–µ–∫—Ç–æ—Ä\n",
        "  - –¢–æ –µ—Å—Ç—å –Ω–∞—à–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Ç–æ–≥–æ —Å–∫–æ–ª—å–∫–æ –ø–∞–¥–æ–≤ —É –Ω–∞—Å –µ—Å—Ç—å –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏\n",
        "  - –ö–æ–≥–¥–∞ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à—É —Å–µ—Ç–∫—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –º—ã –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å –≤ –Ω–µ–µ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É, –≥–¥–µ –ø–∞–¥–æ–≤ –Ω–µ –±—É–¥–µ—Ç\n",
        "    - –¢–æ –µ—Å—Ç—å –ø–æ–ª—É—á–∞–µ—Ç—Å—è –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å –Ω–µ –≤ —Ç–æ–π –∂–µ —Å—Ä–µ–¥–µ, –∫–∞–∫ –∏ –æ–±—É—á–∞–ª–∏\n",
        "      - –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞—à–∏ –≤—Ö–æ–¥—ã –º–µ–Ω—è—é—Ç—Å—è, –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–¥—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –¥—Ä—É–≥–æ–π\n",
        "    - –≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è `distribution shift`, —Ç–æ –µ—Å—Ç—å –∫–æ–≥–¥–∞ –º—ã —É—á–∏–º—Å—è –Ω–∞ –æ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞ –¥—Ä—É–≥–∏—Ö\n",
        "      - –≠—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–ª–æ—Ö–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –∏–Ω–æ–≥–¥–∞ —Ç–æ–ª—å–∫–æ —Ç–∞–∫ –º—ã –∏ –º–æ–∂–µ–º —É—á–∏—Ç—å—Å—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –Ω—É–∂–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞\n",
        "      - –≠—Ç–æ –ø–ª–æ—Ö–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –º—ã –≤–Ω–æ—Å–∏–º —ç—Ç–æ \"—Å–ª—É—á–∞–π–Ω–æ\", –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ–º, —Ç–æ –µ—Å—Ç—å —ç—Ç–æ —Å–≤–æ–µ–±—Ä–∞–∑–Ω—ã–π –±–∞–≥\n"
      ],
      "id": "dbc96104"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123324f0"
      },
      "source": [
        "## –î–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "- –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫—É(–∫–∏) –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ—á–µ–º—É –æ–Ω–∞(–æ–Ω–∏)\n",
        "    - –û–±—ã—á–Ω–æ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏—è –∫–∞–∫–∏–µ –≤–µ—Å–∞ –±—Ä–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø–æ–º–æ–≥—É—Ç –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ —Ç–æ–º –≤—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –Ω–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, —Ö–æ—Ä–æ—à–æ –ª–∏ –º–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ –∏ —Ç–¥\n",
        "- –≠—Ç—É –¥–æ–º–∞—à–∫—É –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∏ –Ω–∞ `CPU`, –Ω–æ –Ω–∞ `GPU` –±—É–¥–µ—Ç —Å–∏–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ\n",
        "    - –í–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–æ–º–∞—à–∫–∞—Ö –º—ã –±—É–¥–µ–º —É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ `GPU`\n",
        "    - –†–∞–Ω–æ –∏–ª–∏ –ø–æ–∑–¥–Ω–æ –≤–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —ç—Ç–æ—Ç [—Ç—É—Ç–æ—Ä–∏–∞–ª](https://www.youtube.com/watch?v=pgk1zGv5lU4)\n",
        "    - –í—ã –º–æ–∂–µ—Ç–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ `colab`, —ç—Ç–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ\n",
        "\n",
        "## –î–æ —ç–ø–æ—Ö–∏\n",
        "- –°–¥–µ–ª–∞–π—Ç–µ —Å–ø–∏—Å–∫–∏/—Å–ª–æ–≤–∞—Ä–∏/–¥—Ä—É–≥–æ–µ, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫(–∏) –ø–æ –≤—Å–µ–π —ç–ø–æ—Ö–µ –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "\n",
        "## –í–æ –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ [`tqdm`](https://github.com/tqdm/tqdm) –∫–∞–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –∫–∞–∫ –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤–∞—à–µ –æ–±—É—á–µ–Ω–∏–µ\n",
        "- –õ–æ–≥–∏—Ä—É–π—Ç–µ –ª–æ—Å—Å\n",
        "- –õ–æ–≥–∏—Ä—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫—É(–∫–∏) –ø–æ –±–∞—Ç—á—É\n",
        "- –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫(–∏) –Ω–∞ –≤—Å—é —ç–ø–æ—Ö—É –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "\n",
        "## –ü–æ—Å–ª–µ —ç–ø–æ—Ö–∏\n",
        "- –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫(–∏) –Ω–∞ –≤—Å—é —ç–ø–æ—Ö—É –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "\n",
        "## –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
        "- –ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
        "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏:\n",
        "    - [Confusion Matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix)\n",
        "    - [–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—Ç–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∫–æ–≥–æ-—Ç–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã –≤—ã–±–∏—Ä–∞–µ–º —Ç–∞–∫–æ–π –∫–ª–∞—Å—Å –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–∞–∫–∞—è-—Ç–æ) –Ω–∞ —Ç—Ä–µ–π–Ω–µ/—Ç–µ—Å—Ç–µ/–≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "        - –ï—Å–ª–∏ –∫–ª–∞—Å—Å –±—ã–ª –≤—ã–±—Ä–∞–Ω –≤–µ—Ä–Ω–æ –∏ –µ—Å–ª–∏ –±—ã–ª–∞ –æ—à–∏–±–∫–∞\n",
        "- –ü–æ–¥—É–º–∞–π—Ç–µ —á—Ç–æ –µ—â–µ –≤–∞–º –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Ç–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã: \n",
        "    - –ß—Ç–æ –≤ –º–æ–¥–µ–ª–µ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å?\n",
        "    - –í—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –º–æ–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏?\n",
        "    - –í—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π?\n",
        "    - –ù–µ –ø–µ—Ä–µ–æ–±—É—á–∏–ª—Å—è –ª–∏ —è?\n",
        "    - –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —è –ø–æ—Å–º–æ—Ç—Ä–µ–ª –Ω–∞ –¥–∞–Ω–Ω—ã–µ?\n",
        "    - –ù—É–∂–Ω–æ –ª–∏ –º–Ω–µ —É–ª—É—á—à–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö?\n",
        "    - –ù—É–∂–Ω–æ –ª–∏ –ø–æ–º–µ–Ω—è—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏?\n",
        "    - –ù–µ—Ç –ª–∏ —É –º–µ–Ω—è –±–∞–≥–æ–≤ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏?\n",
        "    - –ö–∞–∫–∏–µ —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ —É –º–æ–µ–π –º–æ–¥–µ–ª–∏?\n",
        "    - –ö–∞–∫ —è –º–æ–≥—É –∏—Ö –∏—Å–ø—Ä–∞–≤–∏—Ç—å?"
      ],
      "id": "123324f0"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a067d717",
        "outputId": "811a8498-4a7a-4741-9f2e-bc374551e84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading embeddings file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:09<00:00, 10378.96it/s]\n"
          ]
        }
      ],
      "source": [
        "token2index, embeddings = load_embeddings(\"cc.ru.300.vec\")"
      ],
      "id": "a067d717"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "da42260c"
      },
      "outputs": [],
      "source": [
        "class DeepAverageNetwork(nn.Module):\n",
        "    # out_dim = 3, –ø–æ—Ç–æ–º—É —á—Ç–æ 3 –∫–ª–∞—Å—Å–∞: –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
        "    def __init__(self, emb_dim, embeddings = torch.Tensor(embeddings), out_dim = 3, token2index = token2index) -> None:\n",
        "        super().__init__()\n",
        "        self.token2index = token2index\n",
        "        self.emb_dim = emb_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.vocab_size = len(token2index)\n",
        "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_dim).from_pretrained(embeddings)\n",
        "        \n",
        "        self.linear1 = torch.nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.linear2 = torch.nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.norm = torch.nn.BatchNorm1d(self.emb_dim)\n",
        "        self.dropout = torch.nn.Dropout()\n",
        "        self.activate = torch.nn.Softmax(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = self.norm(x)\n",
        "        x = self.relu(self.linear1(norm))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.linear2(x)) + norm\n",
        "        x = self.activate(x)\n",
        "        return x"
      ],
      "id": "da42260c"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7966f715"
      },
      "outputs": [],
      "source": [
        "model = DeepAverageNetwork(300)"
      ],
      "id": "7966f715"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b4a2161",
        "outputId": "029a5f04-56a0-4a2c-e72e-80e6f3440189"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepAverageNetwork(\n",
              "  (embeddings): Embedding(100000, 300)\n",
              "  (linear1): Linear(in_features=300, out_features=300, bias=True)\n",
              "  (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (activate): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model"
      ],
      "id": "4b4a2161"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5efad1d1"
      },
      "source": [
        "## –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä"
      ],
      "id": "5efad1d1"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "34c552fa"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "id": "34c552fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628847fe"
      },
      "source": [
        "## –°–¥–µ–ª–∞–π—Ç–µ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"
      ],
      "id": "628847fe"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b634b9ce",
        "outputId": "93809c43-f889-4c4c-830a-dc068e5b7ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running epoch 0, best test loss: 100000.0 on epoch -1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22808/22808 [00:29<00:00, 767.93it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6142/6142 [00:06<00:00, 888.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, tr_loss 25.103817397305594, te_loss 254.56480364487405\n",
            "Running epoch 1, best test loss: 254.56480364487405 on epoch -1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22808/22808 [00:31<00:00, 713.77it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6142/6142 [00:05<00:00, 1024.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, tr_loss 25.148217200697808, te_loss 221.69678385395136\n",
            "Running epoch 2, best test loss: 221.69678385395136 on epoch -1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22808/22808 [00:29<00:00, 779.77it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6142/6142 [00:06<00:00, 1017.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, tr_loss 25.20002547205971, te_loss 87.43389377575362\n",
            "Running epoch 3, best test loss: 87.43389377575362 on epoch -1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22808/22808 [00:30<00:00, 748.01it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6142/6142 [00:06<00:00, 1023.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, tr_loss 25.12268932206641, te_loss 85.26144983061204\n",
            "Running epoch 4, best test loss: 85.26144983061204 on epoch -1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22808/22808 [00:29<00:00, 777.82it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6142/6142 [00:06<00:00, 1018.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, tr_loss 25.242419897533207, te_loss 79.21180605849548\n",
            "Validating...\n",
            "val_loss = 80.4844709854126\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 5  # –ó–∞–¥–∞–π—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
        "# –î–æ–±–∞–≤–∏—Ç—å .to(device) —á—Ç–æ–±—ã —É—á–∏—Ç—å –Ω–∞ –≥–ø—É?\n",
        "best_epoch = -1\n",
        "best_test_loss = 1e5\n",
        "f1_score = -1e5\n",
        "for n_epoch in range(NUM_EPOCHS):\n",
        "    # TRAIN\n",
        "    print(f\"Running epoch {n_epoch}, best test loss: {best_test_loss} on epoch {best_epoch}\")\n",
        "    step = 0\n",
        "    tr_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(train_loader)\n",
        "    for i, (x, y) in enumerate(pbar):\n",
        "        if len(y) < 2:\n",
        "            continue\n",
        "        step += 1\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        y_hat = torch.Tensor(y_pred.detach().numpy().argmax(1).tolist())\n",
        "        y_hat.requires_grad_()\n",
        "        loss = criterion(y, y_hat)\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    tr_loss /= step\n",
        "    \n",
        "    # TEST\n",
        "    step = 0\n",
        "    te_loss = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pbar = tqdm(test_loader)\n",
        "        for i, (x, y) in enumerate(pbar):\n",
        "            if len(y) < 2:\n",
        "                continue\n",
        "            step += 1\n",
        "            y_pred = model(x)\n",
        "            y_hat = torch.Tensor(y_pred.detach().numpy().argmax(1).tolist())\n",
        "            y_hat.requires_grad_()\n",
        "            loss = criterion(y, y_hat)\n",
        "            te_loss += loss.item()\n",
        "        te_loss /= step\n",
        "\n",
        "    if te_loss < best_test_loss:\n",
        "        best_test_loss = te_loss\n",
        "        best_ep = n_epoch\n",
        "        # torch.save(model.state_dict(), f\"best_model.pt\")\n",
        "    print(f\"epoch {n_epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
        "\n",
        "\n",
        "    \n",
        "# VALIDATE\n",
        "print(\"Validating...\")\n",
        "step = 0\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for i, (x, y) in enumerate(valid_loader):\n",
        "        if len(y) < 2:\n",
        "            continue\n",
        "        step += 1\n",
        "        y_pred = model(x)\n",
        "        y_hat = torch.Tensor(y_pred.detach().numpy().argmax(1).tolist())\n",
        "        y_hat.requires_grad_()\n",
        "        loss = criterion(y, y_hat)\n",
        "        val_loss += loss.item()\n",
        "    val_loss /= step\n",
        "print(f\"val_loss = {val_loss}\")\n",
        "        \n",
        "\n"
      ],
      "id": "b634b9ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ad7f80"
      },
      "outputs": [],
      "source": [],
      "id": "c3ad7f80"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('cv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "c881db0e76e4d07227a54605e4579ac40ea89326ca18881e5cf3019b0657001f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}